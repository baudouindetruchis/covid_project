# -*- coding: utf-8 -*-
"""agre_pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1loDXQLdmyPRSOUKVg-87TUITXfQmXymm
"""

import scipy.io
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

import tensorflow as tf

import keras
from keras.preprocessing import image
from keras.callbacks import ModelCheckpoint,EarlyStopping
from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation
from keras.layers import Conv2D, AveragePooling2D
from keras.models import Model, Sequential

from sklearn.model_selection import train_test_split

from keras import metrics

from keras.models import model_from_json
import matplotlib.pyplot as plt

from shutil import unpack_archive

#%%
# Pour dézipper le dataset "wiki_crop.tar" si besoin
#unpack_archive('D:/Prog/Anaconda/workspace/age_prediction/wiki_crop.tar', 'destination_path/')

#%%

# ne pas oublier d'adapter tous les chemins lorsque vous lancez le code
# de votre côté
mat = scipy.io.loadmat('D:/Prog/Anaconda/workspace/age_prediction/destination_path/wiki_crop/wiki.mat')

instances = mat['wiki'][0][0][0].shape[1]
 
columns = ["dob", "photo_taken", "full_path", "gender", "name", "face_location", "face_score", "second_face_score"]
 
df = pd.DataFrame(index = range(0,instances), columns = columns)

for i in mat:
    if i == "wiki":
        current_array = mat[i][0][0]
        for j in range(len(current_array)):
            df[columns[j]] = pd.DataFrame(current_array[j][0])

def datenum_to_datetime(datenum):
    days = datenum % 1
    hours = days % 1 * 24
    minutes = hours % 1 * 60
    seconds = minutes % 1 * 60
    exact_date = datetime.fromordinal(int(datenum)) \
    + timedelta(days=int(days)) + timedelta(hours=int(hours)) \
    + timedelta(minutes=int(minutes)) + timedelta(seconds=round(seconds)) \
    - timedelta(days=366)
 
    return exact_date.year
 
df['date_of_birth'] = df['dob'].apply(datenum_to_datetime)

df.head()

df['age'] = df['photo_taken'] - df['date_of_birth']

#remove pictures does not include face
df = df[df['face_score'] != -np.inf]

#some pictures include more than one face, remove them
df = df[df['second_face_score'].isna()]

#check threshold
df = df[df['face_score'] >= 3]

df = df.drop(columns = ['name','face_score','second_face_score','date_of_birth','face_location'])

#some guys seem to be greater than 100. some of these are paintings. remove these old guys
df = df[df['age'] <= 100]

#some guys seem to be unborn in the data set
df = df[df['age'] > 0]

# counts = df['age'].value_counts()
# counts[counts > 600]
df.count

df_tri1 = df.loc[df.age > 72, :]

df_tri2 = df.loc[df.index < df.shape[0] * 0.2, :]

df_final = pd.concat((df_tri1,df_tri2), axis=0)

df_final.count

histogram = df_final['age'].hist(bins=df_final['age'].nunique())

histogram = df['age'].hist(bins=df['age'].nunique())
# Drop les personnes entre ~20 ans et ~60 ans -> Pénalité des classes
# jusqu'à ce que ça passe pour la RAM

df.count

classes = 101 #(0, 100])
print("number of output classes: ",classes)

target_size = (224, 224)

def getImagePixels(image_path):
    img = image.load_img("D:/Prog/Anaconda/workspace/age_prediction/destination_path/wiki_crop/%s" % image_path[0], grayscale=False, target_size=target_size)
    x = image.img_to_array(img).reshape(1, -1)[0]
    #x = preprocess_input(x)
    return x

df_final['pixels'] = df_final['full_path'].apply(getImagePixels)

target = df_final['age'].values
target_classes = keras.utils.to_categorical(target, classes)

#features = df['pixels'].values
features = []

for i in range(0, df_final.shape[0]):
    features.append(df_final['pixels'].values[i])

features = np.array(features)
features = features.reshape(features.shape[0], 224, 224, 3)

features.shape

features /= 255 #normalize in [0, 1]

train_x, test_x, train_y, test_y = train_test_split(features, target_classes
                                        , test_size=0.30)#, random_state=42), stratify=target_classes)

# import gc
# del df
# del df_final
# gc.collect()

#VGG-Face model
model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(Convolution2D(4096, (7, 7), activation='relu'))
model.add(Dropout(0.5))
model.add(Convolution2D(4096, (1, 1), activation='relu'))
model.add(Dropout(0.5))
model.add(Convolution2D(2622, (1, 1)))
model.add(Flatten())
model.add(Activation('softmax'))

#model.summary()

#pre-trained weights of vgg-face model. 
#you can find it here: https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing
#related blog post: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/


model.load_weights('D:/Prog/Anaconda/workspace/age_prediction/vgg_face_weights.h5')

#freeze all layers of VGG-Face except last 7 one
for layer in model.layers[:-7]:
    layer.trainable = False

base_model_output = Convolution2D(classes, (1, 1), name='predictions')(model.layers[-4].output)
base_model_output = Flatten()(base_model_output)
base_model_output = Activation('softmax')(base_model_output)

age_model = Model(inputs=model.input, outputs=base_model_output)

#check trainable layers
if False:
    for layer in model.layers:
        print(layer, layer.trainable)
    
    print("------------------------")
    for layer in age_model.layers:
        print(layer, layer.trainable)

from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rotation_range = 10,
                             zoom_range = [1.5, 2],
                             width_shift_range = 0.1,
                             height_shift_range = 0.1,
                             horizontal_flip = True,
                             vertical_flip = False)

for X_batch, y_batch in datagen.flow(train_x, train_y, batch_size=9):
    # create a grid of 3x3 images
    for i in range(0, 9):
        plt.subplot(330 + 1 + i)
        plt.imshow(X_batch[i], cmap=plt.get_cmap('gray'))
    # show the plot
    plt.show()
    break

sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)

age_model.compile(loss='categorical_crossentropy'
                  , optimizer=keras.optimizers.Adam()
                  #, optimizer = sgd
                  , metrics=['accuracy']
                 )

checkpointer = ModelCheckpoint(
    filepath='classification_age_model.hdf5'
    , monitor = "val_loss"
    , verbose=1
    , save_best_only=True
    , mode = 'auto'
)

scores = []

enableFit = True

if enableFit:
    epochs = 1000
    batch_size = 256

    for i in range(epochs):
        print("epoch ",i)
        
        ix_train = np.random.choice(train_x.shape[0], size=batch_size)
        
        score = age_model.fit_generator(datagen.flow(train_x[ix_train],train_y[ix_train]), 
                                      validation_data = (test_x, test_y),
                                      epochs = 1,
                                      verbose = True,
                                      callbacks=[checkpointer])

        # score = age_model.fit(
        #     train_x[ix_train], train_y[ix_train]
        #     , epochs=1
        #     , validation_data=(test_x, test_y)
        #     , callbacks=[checkpointer]
        # )
        
        scores.append(score)
    
    #restore the best weights
    from keras.models import load_model
    age_model = load_model("classification_age_model.hdf5")
    
    age_model.save_weights('age_model_weights.h5')
        
else:
    #pre-trained weights for age prediction: https://drive.google.com/file/d/1YCox_4kJ-BYeXq27uUbasu--yz28zUMV/view?usp=sharing
    age_model.load_weights("D:/Prog/Anaconda/workspace/age_prediction/age_model_weights.h5")

val_loss_change = []; loss_change = []
for i in range(0, len(scores)):
    val_loss_change.append(scores[i].history['val_loss'])
    loss_change.append(scores[i].history['loss'])

plt.plot(val_loss_change, label='val_loss')
plt.plot(loss_change, label='train_loss')
plt.legend(loc='upper right')
plt.show()

#loss and accuracy on validation set
age_model.evaluate(test_x, test_y, verbose=1)

predictions = age_model.predict(test_x)

output_indexes = np.array([i for i in range(0, 101)])
apparent_predictions = np.sum(predictions * output_indexes, axis = 1)


### Partie prédiction pour tester le modèle (autant voir le script
### age_prediction_final.py dans ce cas)

#from keras.preprocessing import image
#from keras.preprocessing.image import ImageDataGenerator
#
#def loadImage(filepath):
#    test_img = image.load_img(filepath, target_size=(224, 224))
#    test_img = image.img_to_array(test_img)
#    test_img = np.expand_dims(test_img, axis = 0)
#    test_img /= 255
#    return test_img
#
#picture = "destination_path/nm0000069_rm3051211008_1915-12-12_1980.jpg" 
#picture = "destination_path/nm0000169_rm2291440384_1946-9-15_2005.jpg"
#picture = "destination_path/nm0000369_rm44011008_1964-2-18_2009.jpg" 
#
#prediction = age_model.predict(loadImage(picture))
#
#y_pos = np.arange(101)
#plt.bar(y_pos, prediction[0], align='center', alpha=0.3)
#plt.ylabel('percentage')
#plt.title('age')
#plt.show()
#
#img = image.load_img(picture)#, target_size=(224, 224))
#plt.imshow(img)
#plt.show()
#
#print("most dominant age class (not apparent age): ",np.argmax(prediction))
#
#apparent_age = np.round(np.sum(prediction * output_indexes, axis = 1))
#print("apparent age: ", int(apparent_age[0]))



### Partie pour sauvegarder son model sur le dossier root de son Drive perso

#from google.colab import auth
#from googleapiclient.http import MediaFileUpload
#from googleapiclient.discovery import build
#
#auth.authenticate_user()
#
#drive_service = build('drive', 'v3')
#
#def save_file_to_drive(name, path):
#    file_metadata = {
#      'name': name,
#      'mimeType': 'application/octet-stream'
#     }
#    media = MediaFileUpload(path, 
#                             mimetype='application/octet-stream',
#                             resumable=True)
#
#    created = drive_service.files().create(body=file_metadata,
#                                   media_body=media,
#                                   fields='id').execute()
#    print('File ID: {}'.format(created.get('id')))
#
#    return created
#
#save_file_to_drive('classification_age_model_v2.hdf5', 'classification_age_model.hdf5')
#
#save_file_to_drive('age_model_weights_v2.h5', 'age_model_weights.h5')
#
